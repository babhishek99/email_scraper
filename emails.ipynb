{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "import openpyxl\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Linkedin', 'Google']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Company list can be imported from excel or hardcoded as a list\n",
    "company_lst = list(pd.read_excel(\"Company List.xlsx\")['Company'])\n",
    "#Change this list according to titles you want to scrape for\n",
    "titles = [\"Product Manager\", \"Optimization Engineer\", \"Site Reliability Engineer\", \"Infrastructure Engineer\", \n",
    "          \"Light Ops Engineer\", \"Cyber Reliability Engineer\", \"Finance\"]\n",
    "company_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "workbook = openpyxl.Workbook()\n",
    "#Change this path based on what you want the excel file to be\n",
    "path = \"Sourcing-Smitha-Example.xlsx\"\n",
    "wb_act = workbook.active\n",
    "wb_act.title = \"Emails\"\n",
    "workbook.save(path)\n",
    "wb_act['A1'] = \"Name\"\n",
    "wb_act['B1'] = \"Position\"\n",
    "wb_act['C1'] = \"Company\"\n",
    "curr_row = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def secondary_security(browser):\n",
    "    try:\n",
    "        button = browser.find_element_by_class_name(\"secondary-action\")\n",
    "        button.click()\n",
    "    except Exception as e:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_locator(elements, field):\n",
    "    elements = [element for element in elements if element.get_attribute(\"type\") == \"text\"]\n",
    "    for element in elements:\n",
    "        if field in element.get_attribute(\"placeholder\"):\n",
    "            return element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def button_locator(elements, field):\n",
    "    for element in elements:\n",
    "        if field == element.get_attribute(\"data-control-name\"):\n",
    "            return element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape(driver, url, row, company, page_limit=6):\n",
    "    for page in range(2, page_limit + 1): #change to desired page range \n",
    "        page_url = \"\"\n",
    "        if page == 1:\n",
    "            page_url = url\n",
    "        else:\n",
    "            page_url = title_url + \"&page=\" + str(page)\n",
    "        print(\"Scraping this URL: \" + page_url)\n",
    "        driver.get(page_url)\n",
    "        time.sleep(2)\n",
    "        \n",
    "        for _ in range(5):\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight/5);\")\n",
    "            while not page_has_loaded(browser):\n",
    "                pass    \n",
    "            source_code = driver.page_source\n",
    "            soup = BeautifulSoup(source_code)\n",
    "            names = soup.findAll(\"span\", {\"class\" : \"name actor-name\"})\n",
    "            titles_location = soup.findAll(\"span\", {\"dir\" : \"ltr\"})\n",
    "            titles = titles_location[::2]\n",
    "        \n",
    "        iterator = zip(names, titles)\n",
    "        seen_names = set()\n",
    "        for name, position in iterator:\n",
    "            if name in seen_names:\n",
    "                continue\n",
    "            seen_names.add(name.contents[0])\n",
    "            print(name.contents[0] + \"--->\" + position.contents[0])\n",
    "            wb_act['A' + str(row)] = name.contents[0]\n",
    "            wb_act['B' + str(row)] = position.contents[0]\n",
    "            wb_act['C' + str(row)] = company\n",
    "            row += 1\n",
    "        \n",
    "        workbook.save(path)\n",
    "        \n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def page_has_loaded(driver):\n",
    "    page_state = driver.execute_script('return document.readyState;')\n",
    "    return page_state == 'complete'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LINKEDIN AUTHENTICATION\n",
    "\n",
    "#Enter your Linkedin Username and Password here\n",
    "\n",
    "username_auth = \"\"\n",
    "password_auth = \"\"\n",
    "\n",
    "while True:\n",
    "    try: \n",
    "        browser = webdriver.Chrome('chromedriver.exe')\n",
    "        browser.implicitly_wait(30)\n",
    "        browser.get(\"https://www.linkedin.com\")\n",
    "        browser.find_element_by_class_name(\"nav__button-secondary\").click()\n",
    "        username = browser.find_element_by_id(\"username\")\n",
    "        username.send_keys(username_auth)\n",
    "        password = browser.find_element_by_id(\"password\")\n",
    "        password.send_keys(password_auth)\n",
    "        password.submit()\n",
    "        break\n",
    "    except Exception as e:\n",
    "        browser.close()\n",
    "\n",
    "# secondary_security(browser)\n",
    "for company in company_lst:\n",
    "    #FILTER PAGE\n",
    "    browser.get(\"https://www.linkedin.com/search/results/people/?origin=DISCOVER_FROM_SEARCH_HOME\")\n",
    "\n",
    "    #ALL FILTERS BUTTON\n",
    "    time.sleep(3)\n",
    "    button_elements = browser.find_elements_by_tag_name(\"button\")\n",
    "    all_filters_button = button_locator(button_elements, \"all_filters\")\n",
    "    all_filters_button.click()\n",
    "\n",
    "    #ELEMENTS\n",
    "    input_elements = browser.find_elements_by_tag_name(\"input\")\n",
    "    button_elements = browser.find_elements_by_tag_name(\"button\")\n",
    "\n",
    "    # FILTER LOCATIONS\n",
    "    location_elem = input_locator(input_elements, \"country/region\")\n",
    "    locations = [\"United States\", \"San Francisco Bay Area\"]\n",
    "    for location in locations:\n",
    "        location_elem.send_keys(location)\n",
    "        time.sleep(1)\n",
    "        location_elem.send_keys(Keys.DOWN)\n",
    "        location_elem.send_keys(Keys.ENTER)\n",
    "        time.sleep(1)\n",
    "\n",
    "    #FILTER CURRENT COMPANY\n",
    "    company_elem = input_locator(input_elements, \"current company\")\n",
    "    company_elem.send_keys(company)\n",
    "    time.sleep(1)\n",
    "    company_elem.send_keys(Keys.DOWN)\n",
    "    company_elem.send_keys(Keys.ENTER)\n",
    "    time.sleep(1)\n",
    "\n",
    "    #APPLY FILTERS\n",
    "    apply_button = button_locator(button_elements, \"all_filters_apply\")\n",
    "    apply_button.click()\n",
    "\n",
    "    #BASE FILTER URL\n",
    "    time.sleep(3)\n",
    "    filter_url = browser.current_url\n",
    "    print(\"Filter URL is: \" + filter_url)\n",
    "\n",
    "    #URL FOR EACH TITLE\n",
    "    for title in titles:\n",
    "        title_url = filter_url + \"&title=\" + title\n",
    "        curr_row = scrape(browser, title_url, curr_row, company)\n",
    "\n",
    "workbook.close()\n",
    "print(\"Linkedin email scraping is complete.\")\n",
    "print(\"Your file is located at this path from the current directory: \" + path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
